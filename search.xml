<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SAFNet]]></title>
    <url>%2F2019%2F03%2F29%2FSAFNet%2F</url>
    <content type="text"><![CDATA[简介 论文名称：《Dual path multi-scale fusion networks with attention for crowd counting》 CNN分类 基于attention 输入数据的分类 基于完整图像 个人收获 两个相同的网络，一个生成mask，通过attention去除背景，另外一个生成density map，最后两者想乘，得到最后的结果。解决的问题就是树，灯等背景会被识别成人 观点引入Problem:Estimation crowd density map. Existing two major difficult problems: large head scale variations caused by camera perspective and diverse crowd distributions with high background noisy scenes.Goal:To further tackle the high background noise issue, we adopt another path of multi-scale feature fusion as attention map path (AMP) with the same structure to learn a probability map that indicates high probability head regions. Then this attention map is used to suppress non-head regions in the last feature maps of DMP, which makes DMP focus on learning the regression task only in high probability head regions. We also introduce a multi-task loss by adding a attention map loss for AMP, which improves the network performance with more explicit supervised signal.(最后一句没看懂，前面的意思是通过一个同样的网络attentionnet来识别出高概率是人头的地图，然后利用这个高概率的人头地图在网络的最后一层去掉那些没有人头的地方的干扰)思想总结：用att提取出了高概率的人头区域，然后让网络关注人头区域多的位置。多尺度，结合了层次不同但是分辨率相同的feature。 网络结构 Feature map extractor(FME)We choose a pre-trained VGG16 with batch normalization as the frond3 end feature map extractor due to its strong feature represent ability and easily to be concatenated by the back-end dual path networks. The first 13 layers from conv1-1 to conv5- 3 are involved to output feature maps with sizes 1/2, 1/4, 1/8 and 1/16 of the original input size. Four source layers, conv2-2, conv3-3, conv4-3 and conv5-3, which represent multi-scale features and multi-level semantic information, will be concatenated by both DMP and AMP Density map path(DMP)The DMP of SFANet is constructed in feature pyramid structure as illustrated in Fig.2. Conv5-3 feature maps firstly is upsampled by factor 2, and then concatenate feature maps of conv4-3. The detail of transfer connection block T is shown as Fig.3, which contains concat, conv1×1×256, conv3×3×256 and upsample sub-layers. The second T block has the similar structure concatenating conv3-3 with only different channel size 128, that is concat, conv1×1×128, conv3×3×128 and upsample. Then concatenated outputs of second T block and conv2-2 are feed into header block H with concat, conv1×1×64, conv3×3×64 and conv3×3×32 shown as Fig.3. Every 1 × 1 convolution before the 3 × 3 is used to reduce the computational complexity. Due to previous three upsample layers, we can retrieve the final high resolution feature maps with 1/2 size of the original input. Then element-wise multiple is applied on attention map and the last density feature maps to generate refined density feature maps Fref ine as equation 1: Fref ine = fden ⊗ MAtt (1) where fden is the last density features, MAtt is attention map, ⊗ denotes element-wise multiply. Before this operation, MAtt is expanded as the same channel as fden. At last, we use a simple convolution with kernel 1×1×1 to generate the high-quality density map Mden. Batch normalization is applied after every convolutional layer because we find that batch training and batch normalization can stabilize the training process and accelerate loss convergence. We also apply Relu after every convolutional layer except the last one. Attention map path(AMP)The AMP of SFANet has the similar structure with DMP, and output probability map to indicate head probability in each pixel of the density feature map. In this work, we introduce the attention model as follows. Suppose convolutional features output by head block as fatt, the attention map MAtt is generated as: MAtt = Sigmoid(W c fatt + b) (2) where W, b is the 1 × 1 × 1 convolution layer weights and bias, c denotes the convolution operation and Sigmoid denotes the sigmoid activation function. The sigmoid activation function gives out (0, 1) probability scores to make network discriminate head location and background. The visualization of Matt can be seen in Fig.4. The proposed attention map loss will be further discussed the next section. loss]]></content>
  </entry>
  <entry>
    <title><![CDATA[Learning to count objects in images]]></title>
    <url>%2F2018%2F08%2F01%2FLearning%20To%20Count%20Objects%20in%20Images%2F</url>
    <content type="text"><![CDATA[简介 论文名称：《Learning to count objects in images》 出处：NIPS 2010 个人收获 想通过本文了解问什么采用高斯核计数，但是还是没能理解其中的真谛哎 提出了用高斯核生成真值。提出了一种距离计算方法，并将其转化为convex quadratic problem 摘要文中提出一种新的有监督学习的框架，用于视觉计数任务如显微镜下的细胞计数，监控下的人群计数。我们重点关注训练图片为标记为点的情况。（一个点代表一个实体）目标是准确计数。论文没有采用监测和标记具体事例的方法。而是把问题转化为评估一个图像的密度图，通过对密度图积分（就是加和）即可得到整张图片的计数。学习推断这样的密度可以被制定为，正则化风险二次损失函数的最小化。文中引入一种新的损失函数来适配这种学习，与此同时可以用通过最大子数组问题（maximum subarray algorithm）的方式求解。然后可以把学习当作凸二次规划可解压缩平面优化。提议的框架非常灵活，因为它可以接受任何领域特定的可视化。一旦训练，我们的系统提供精确的对象计数而且特征提取上的时间开销也很少，使它特别适合需要处理实时数据或需要处理大量可视化数据的应用程序。 #introduction前面说的跟摘要类似，就是计数选的是人标记一个点，很符合人类的计数习惯并且省力。文中针对这种真值是标点的问题，提出相应的解决办法。并给出一些如何利用除了标点意外的信息。问题定义：对于给定图片 I，给出一个密度函数F作为一个为途中的每一个像素计算真值的函数。假设用xp每一个图像中的像素 p 的特征，密度函数可以认为是一个线性变换： xp: F(p) = wT xp. 对于给定的一组训练图片，参数向量 W 通过 the regularized risk framework学习获得, 所以 对训练图片的密度函数估计匹配通过人为标记密度真值(通过在W上的正则化) . （这段话没看懂哦） 相关工作 检测，递归两种方法 Framework真值密度函数可以看成一个基于给定点的 核密度评估。 具体的是根据文中标记为1的点，生成相应的高斯核，最后，累计图片中的数字，就是总的计数。文中用matlab 实现，filter 大小是15，方差为4。 一直不太明白这样做的具体意义是什么。特别是filter的大小 该如何选择。（留白吧，希望后面理解了回来补一下）后面的算法是传统算 就不翻译了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[body structure aware deep crowd counting]]></title>
    <url>%2F2018%2F07%2F07%2Fbody-structure-aware-deep-crowd-counting%2F</url>
    <content type="text"><![CDATA[简介 论文名称：《Body Structure Aware Deep Crowd Counting》 CNN分类 基础CNN 上下文感知 多任务模型 输入数据的分类 基于完整图像 个人收获 通过引入人的具体形状来避免了树叶，还有灯灯对人群计数的混淆; 在没有perspective map的情况下手动标记一张，然后应用全部，这是论文选择数据集的一个标准； 观点引入影响人流统计的三个因素：行人、头、上下文信息（context structure）。2016 CVPR, 2017 CVPR 只考虑了行人因素（这个观点不能赞同，神经网络应该会自动提取上下文信息的），没有加入上下文信息。文中通过语义分割的观点来实现人流统计。两部分场景解析模型：第一部分为身体部分图，用来标记身体的部分，如图1，用不同的颜色标记出人的头，上身，和腿，文中接采用的是[2013 iccv Pedestrain parsing via deep decompositional network]提出的网络，及训练好的参数。第二部分为结构人群密度图。利用身体部分图产生的各个行人的详细形状，构建结构化的人群密度图。 本文采用多任务学习，具体分为以下三个任务：1，2如上描述的语义分割模型 3.评估人群数。 相关工作 人群计数 分为三类: 基于检测的,基于全局递归,基于密度评估的 行人语义分析 卷积神经网络 应用FCN 来实现场景解析模型。（去掉了后面的 全连接网络 改用conv2 1*1 ） 方法问题定义 身体部分图本文应用了视野图（但是很多数据集是没有这个选项的，这也限制了这篇论文的应用场景）。M(p) 表示在位置P上 要用多少个像素表示1米。 一个人的左上角和右下角评估方法如下： 该论文假设人的高约2米内，宽1米内（可以在思考下为什么这么设置，虽然是实验结果）。 结构化密度图结构密度图用于同时捕获密度分布和行人的形状（行人形状这个很好了解决了灯和树叶等问题）。结构化的密度图： 多任务人群计数框架]]></content>
  </entry>
  <entry>
    <title><![CDATA[你好，世界]]></title>
    <url>%2F2018%2F03%2F13%2Fhello-world%2F</url>
    <content type="text"><![CDATA[我的第一篇文章]]></content>
  </entry>
</search>
