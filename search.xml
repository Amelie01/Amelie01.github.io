<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[casacde mask generation framework for fast samll object detection]]></title>
    <url>%2F2019%2F04%2F09%2Fcasacde-mask-generatio-framework-for-fast-samll-object-detection%2F</url>
    <content type="text"><![CDATA[简介小物体的一些基本常识，摘自知乎小目标主要还是三个方面存在问题： 样本数量：数据本身包含的小目标就少，能跟anchor重叠的更少，造成小目标本身证样本数量低，检测效果差。所以数据还有anchor设置很重要 下采样倍率：一般网络速度不能太慢，也就要求下采样倍率不能太低。然而下采样倍数比较高的情况下小目标会因为经过过多次卷积造成过多的信息丢失，导致检测效果差。另一方面，anchor与目标的匹配与下采样倍率直接相关，过低的下采样倍率直接影响小目标与anchor的匹配，造成样本过低，导致检测效果差 输入图片尺度：同上，全卷积网络不要求输入尺寸固定，因此多数目标检测框架输入是可以任意的。可是输入尺寸直接影响网络速度，因此输入图片尺度不可以太大，这样同上，小目标信息损失会相对于大目标更大，同时匹配上的证样本更少，导致检测效果更差Small Object Detection小物体检测可以被认为是物体检测的特殊情况，但是一般物体检测器在检测小物体时可能会遇到一些问题，特征表示不足是小物体检测的主要限制。为了克服这个问题，大致有这些处理方法： 浅层的池化特征用来识别和增强的分类器用来分类 对像素进行上采样以获得更精细的表示 使用多尺度输入来处理不同尺寸的特征 感知生成对抗网络（P-GAN）模型来缩小小对象和大对象之间的表示差异 然而，这些方法仍然在无关背景上花费大量计算资源，作者试图从昂贵的卷积运算中尽可能多地排除背景区域。 因为通过短链接的方式和结合底层特征的方式，crowd counting 已经有人用过了，了解一下放大图像的方向。*主要 论文名称：《CASCADE MASK GENERATION FRAMEWORK FOR FAST SMALL OBJECT DETECTION》 个人收获 两个相同的网络，一个生成mask，通过attention去除背景，另外一个生成density map，最后两者想乘，得到最后的结果。解决的问题就是树，灯等背景会被识别成人 观点引入Problem:A straightforward solution to solve “small object detection frameworks” is to enlarge the input iamge size.As such, the feature size if proportionally enlarged and the feature representation is finer. howerver, the computational cost is also proportionally increased,and even more of it is wasted on the irrelevant background,making hte entire detection process highly inefficient.(就是找出那些需要放大的小目标，从而提高效率，那么问题的难点或者重点就在于怎么定位小物体和无用的背景区域) Goal:they use a low-resolution image estimate a mask that excludes the background regions. then the rest of the regions can be processed at a higher resolution without much cost. they also further adopt a cascade architecture to filter more regions.specifically,the input image is resized into multiple scales,and they are processed in ascending order of scales.For each input scale,a mask is generated for the next scale input in addition to the object proposals.(生成mask,被选中的候选区域会被放大) benefit:（1）it significantly reduces the computational cost.（2）early stages filter easy negatives, so that later stages can focus on hard negative examples. related work Object Detection Framework：region-based detectors andsingle-shot detectors.Small Object Detection：Although small object detection can be considered as a special case of object detection, general object detectors may face some issues when detecting small objects, as Zhang et al. pointed out in [13]. The insufficientfeature representation is the main limitation in small object detection. To overcome this problem, Zhang et al. [13] pool features from shallow layer and adopt a boosted forest classifier. Cai et al. [14] upsample the features to get a finer representation. Hu et al. [15] use multi-scale input to handle faces in different sizes. Li et al. [16] propose a perceptual generative adversarial network (P-GAN) model to narrow the representation difference between small objects and large objects. However, these methods still spend a large amount of computational resources on irrelevant backgrounds.这几篇还要再去看一下目前就简单粘贴了 #their approach ##Framework Overview Following the Faster R-CNN [9], we build a region-based detection system with multi-scale input. The input images are resized to several scales. For the smallest scale, we apply a standard RPN [9] to extract the candidate bounding boxes.Meanwhile, the mask for the next scale is estimated by theMGM. The MGM and RPN share the same base convolutional feature maps. Therefore it only introduces little overhead compared to the standard RPN.(mgm 和 rpn 共享 网络，这样可以提升效率，有点像多任务，也想SAFNet,就是真值的区域基本相同，只是形式不同，这时候就可以共用相同的网络结构)With the mask generated from the previous scale, thebackground regions are discarded when an image is fed intoCNNs. For the rest of regions, we use almost the same implementations as the smallest input: standard RPN and MGM.Note that we do not need to estimate the mask for the last input. （小尺寸的生成mask，大尺寸的只计算mask内的东西）The region proposals are used for post-classification, just like fast R-CNN [8]. We adopt the RoI align method proposed by He et al. [20] to extract regional features from the last convolutional feature maps produced from the largest input. The post-classifier decides which category the candidates belong to. The whole framework is illustrated in Fig. 2. Mask Generation Module将图片切割成32*32个patches,当一个patch和真值有交集的时候，它被激活，全部被激活的patch就是MGM.ground_True坐标映射规则为： 其中，（x1i; y1i）和（x2i; y2i）是第i个box的左上角和右下角的坐标值。没明白为什么要从新映射一下啊直接用坐标算不就行了吗，每懂啊。采用全卷积网络来生成掩码。它将H x W图像作为输入并生成h x w热图。为简单起见，在骨干网络的最后一个卷积层之后添加池化层以产生具有h x w分辨率的特征映射。然后我们添加两个1x1卷积层。其中一个用于提取掩码生成的特征，另一个用于生成概率图。实际上，MGM的结构与RPN几乎相同。唯一的区别是MGM了解区域是否包含目标，而RPN了解目标的位置。 因为他们的大致区域是一样的只是形式不同而已。 对于MGM，使用Softmax损失函数。由于前景和背景之间的不平衡，使用了所有正例，并随机选择一些负例，使样本比例为1：3。(没理解这里做了什么，就是什么叫随机选取了负例)。 所有示例都在可用区域中选择。这意味着如果某些区域没有被包含在先前的掩码，则在后面训练过程中会忽略它们。 Detection moduleThe detection module includes two parts: the RPN and thepost-classifier, similar to the Faster R-CNN [9] framework. Since we use multi-scale inputs, the entire network has multiple RPNs and only one post-classifier. For each RPN, we predefine a proper anchor setting, e.g., the low-resolution RPN is expected to detect large objects but ignore small objects. After region proposal, the candidate regions extract features from the feature maps produced by the largest input and feed them into the post-classifier. As some regions are discarded by the MGM, the RPN does not propose bounding boxes outside of the positive regions. Multi-task Loss Function]]></content>
  </entry>
  <entry>
    <title><![CDATA[SAFNet]]></title>
    <url>%2F2019%2F03%2F29%2FSAFNet%2F</url>
    <content type="text"><![CDATA[简介 论文名称：《Dual path multi-scale fusion networks with attention for crowd counting》 CNN分类 基于attention 输入数据的分类 基于完整图像 个人收获 两个相同的网络，一个生成mask，通过attention去除背景，另外一个生成density map，最后两者想乘，得到最后的结果。解决的问题就是树，灯等背景会被识别成人 观点引入Problem:Estimation crowd density map. Existing two major difficult problems: large head scale variations caused by camera perspective and diverse crowd distributions with high background noisy scenes.Goal:To further tackle the high background noise issue, we adopt another path of multi-scale feature fusion as attention map path (AMP) with the same structure to learn a probability map that indicates high probability head regions. Then this attention map is used to suppress non-head regions in the last feature maps of DMP, which makes DMP focus on learning the regression task only in high probability head regions. We also introduce a multi-task loss by adding a attention map loss for AMP, which improves the network performance with more explicit supervised signal.(最后一句没看懂，前面的意思是通过一个同样的网络attentionnet来识别出高概率是人头的地图，然后利用这个高概率的人头地图在网络的最后一层去掉那些没有人头的地方的干扰)思想总结：用att提取出了高概率的人头区域，然后让网络关注人头区域多的位置。多尺度，结合了层次不同但是分辨率相同的feature。 网络结构 Feature map extractor(FME)We choose a pre-trained VGG16 with batch normalization as the frond3 end feature map extractor due to its strong feature represent ability and easily to be concatenated by the back-end dual path networks. The first 13 layers from conv1-1 to conv5- 3 are involved to output feature maps with sizes 1/2, 1/4, 1/8 and 1/16 of the original input size. Four source layers, conv2-2, conv3-3, conv4-3 and conv5-3, which represent multi-scale features and multi-level semantic information, will be concatenated by both DMP and AMP Density map path(DMP)The DMP of SFANet is constructed in feature pyramid structure as illustrated in Fig.2. Conv5-3 feature maps firstly is upsampled by factor 2, and then concatenate feature maps of conv4-3. The detail of transfer connection block T is shown as Fig.3, which contains concat, conv1×1×256, conv3×3×256 and upsample sub-layers. The second T block has the similar structure concatenating conv3-3 with only different channel size 128, that is concat, conv1×1×128, conv3×3×128 and upsample. Then concatenated outputs of second T block and conv2-2 are feed into header block H with concat, conv1×1×64, conv3×3×64 and conv3×3×32 shown as Fig.3. Every 1 × 1 convolution before the 3 × 3 is used to reduce the computational complexity. Due to previous three upsample layers, we can retrieve the final high resolution feature maps with 1/2 size of the original input. Then element-wise multiple is applied on attention map and the last density feature maps to generate refined density feature maps Fref ine as equation 1: Fref ine = fden ⊗ MAtt (1) where fden is the last density features, MAtt is attention map, ⊗ denotes element-wise multiply. Before this operation, MAtt is expanded as the same channel as fden. At last, we use a simple convolution with kernel 1×1×1 to generate the high-quality density map Mden. Batch normalization is applied after every convolutional layer because we find that batch training and batch normalization can stabilize the training process and accelerate loss convergence. We also apply Relu after every convolutional layer except the last one. Attention map path(AMP)The AMP of SFANet has the similar structure with DMP, and output probability map to indicate head probability in each pixel of the density feature map. In this work, we introduce the attention model as follows. Suppose convolutional features output by head block as fatt, the attention map MAtt is generated as: MAtt = Sigmoid(W c fatt + b) (2) where W, b is the 1 × 1 × 1 convolution layer weights and bias, c denotes the convolution operation and Sigmoid denotes the sigmoid activation function. The sigmoid activation function gives out (0, 1) probability scores to make network discriminate head location and background. The visualization of Matt can be seen in Fig.4. The proposed attention map loss will be further discussed the next section. loss]]></content>
  </entry>
  <entry>
    <title><![CDATA[Learning to count objects in images]]></title>
    <url>%2F2018%2F08%2F01%2FLearning%20To%20Count%20Objects%20in%20Images%2F</url>
    <content type="text"><![CDATA[简介 论文名称：《Learning to count objects in images》 出处：NIPS 2010 个人收获 想通过本文了解问什么采用高斯核计数，但是还是没能理解其中的真谛哎 提出了用高斯核生成真值。提出了一种距离计算方法，并将其转化为convex quadratic problem 摘要文中提出一种新的有监督学习的框架，用于视觉计数任务如显微镜下的细胞计数，监控下的人群计数。我们重点关注训练图片为标记为点的情况。（一个点代表一个实体）目标是准确计数。论文没有采用监测和标记具体事例的方法。而是把问题转化为评估一个图像的密度图，通过对密度图积分（就是加和）即可得到整张图片的计数。学习推断这样的密度可以被制定为，正则化风险二次损失函数的最小化。文中引入一种新的损失函数来适配这种学习，与此同时可以用通过最大子数组问题（maximum subarray algorithm）的方式求解。然后可以把学习当作凸二次规划可解压缩平面优化。提议的框架非常灵活，因为它可以接受任何领域特定的可视化。一旦训练，我们的系统提供精确的对象计数而且特征提取上的时间开销也很少，使它特别适合需要处理实时数据或需要处理大量可视化数据的应用程序。 #introduction前面说的跟摘要类似，就是计数选的是人标记一个点，很符合人类的计数习惯并且省力。文中针对这种真值是标点的问题，提出相应的解决办法。并给出一些如何利用除了标点意外的信息。问题定义：对于给定图片 I，给出一个密度函数F作为一个为途中的每一个像素计算真值的函数。假设用xp每一个图像中的像素 p 的特征，密度函数可以认为是一个线性变换： xp: F(p) = wT xp. 对于给定的一组训练图片，参数向量 W 通过 the regularized risk framework学习获得, 所以 对训练图片的密度函数估计匹配通过人为标记密度真值(通过在W上的正则化) . （这段话没看懂哦） 相关工作 检测，递归两种方法 Framework真值密度函数可以看成一个基于给定点的 核密度评估。 具体的是根据文中标记为1的点，生成相应的高斯核，最后，累计图片中的数字，就是总的计数。文中用matlab 实现，filter 大小是15，方差为4。 一直不太明白这样做的具体意义是什么。特别是filter的大小 该如何选择。（留白吧，希望后面理解了回来补一下）后面的算法是传统算 就不翻译了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[body structure aware deep crowd counting]]></title>
    <url>%2F2018%2F07%2F07%2Fbody-structure-aware-deep-crowd-counting%2F</url>
    <content type="text"><![CDATA[简介 论文名称：《Body Structure Aware Deep Crowd Counting》 CNN分类 基础CNN 上下文感知 多任务模型 输入数据的分类 基于完整图像 个人收获 通过引入人的具体形状来避免了树叶，还有灯灯对人群计数的混淆; 在没有perspective map的情况下手动标记一张，然后应用全部，这是论文选择数据集的一个标准； 观点引入影响人流统计的三个因素：行人、头、上下文信息（context structure）。2016 CVPR, 2017 CVPR 只考虑了行人因素（这个观点不能赞同，神经网络应该会自动提取上下文信息的），没有加入上下文信息。文中通过语义分割的观点来实现人流统计。两部分场景解析模型：第一部分为身体部分图，用来标记身体的部分，如图1，用不同的颜色标记出人的头，上身，和腿，文中接采用的是[2013 iccv Pedestrain parsing via deep decompositional network]提出的网络，及训练好的参数。第二部分为结构人群密度图。利用身体部分图产生的各个行人的详细形状，构建结构化的人群密度图。 本文采用多任务学习，具体分为以下三个任务：1，2如上描述的语义分割模型 3.评估人群数。 相关工作 人群计数 分为三类: 基于检测的,基于全局递归,基于密度评估的 行人语义分析 卷积神经网络 应用FCN 来实现场景解析模型。（去掉了后面的 全连接网络 改用conv2 1*1 ） 方法问题定义 身体部分图本文应用了视野图（但是很多数据集是没有这个选项的，这也限制了这篇论文的应用场景）。M(p) 表示在位置P上 要用多少个像素表示1米。 一个人的左上角和右下角评估方法如下： 该论文假设人的高约2米内，宽1米内（可以在思考下为什么这么设置，虽然是实验结果）。 结构化密度图结构密度图用于同时捕获密度分布和行人的形状（行人形状这个很好了解决了灯和树叶等问题）。结构化的密度图： 多任务人群计数框架]]></content>
  </entry>
  <entry>
    <title><![CDATA[你好，世界]]></title>
    <url>%2F2018%2F03%2F13%2Fhello-world%2F</url>
    <content type="text"><![CDATA[我的第一篇文章]]></content>
  </entry>
</search>
